### Import all needed library
```{r}
library(readr)
library(keras)
library(doParallel)
library(foreach)
library(parallel)
library(e1071)
library(caret)
library(randomForest)
library(sparklyr)
library(dplyr)
library(glmnet)
library(ridge)
library(lars)
library(corrplot)
library(car)
```

### Data Integration
```{r}
files <-
  list.files(path = "data",
             pattern = "*.csv",
             full.names = TRUE)

read_csv <- function(file_path) {
  read.csv(file_path)
}
desired_columns <-
  c(
    "IYEAR",
    "EDUCA",
    "SMOKE100",
    "EXERANY2",
    "MARITAL",
    "NUMADULT",
    "RENTHOM1",
    "HLTHPLN1",
    "EMPLOY1",
    "PVTRESD1",
    "MEDCOST",
    "CHILDREN",
    "INCOME2",
    "MENTHLTH"
  )

read_and_extract <- function(file_path) {
  data <- read.csv(file_path)
  data[desired_columns[desired_columns %in% names(data)]]
}

cl <- makeCluster(detectCores() - 1)

clusterExport(cl, varlist = c("desired_columns", "read_and_extract"))

all_data_list <- parLapply(cl, files, read_and_extract)

stopCluster(cl)

all_data <- do.call(rbind, all_data_list)

write.csv(all_data, "merged_data.csv", row.names = FALSE)
```

### Data management
```{r}
#all_data = read.csv("merged_data.csv")
brfss <- na.omit(all_data)
brfss <- brfss[brfss$MENTHLTH <= 70,]
brfss <- brfss[brfss$CHILDREN <= 20,]
brfss <- brfss[brfss$INCOME2 <= 70,]
brfss$MENTHLTH <-
  cut(
    brfss$MENTHLTH,
    breaks = c(1, 5, 10, 15, 20, 25, Inf),
    labels = c("1", "2", "3", "4", "5", "6"),
    right = FALSE
  )

summary(brfss$MENTHLTH)

brfss$IYEAR <- gsub("b'", "", brfss$IYEAR)
brfss$IYEAR <- gsub("'", "", brfss$IYEAR)
brfss$IYEAR <- as.integer(brfss$IYEAR)
brfss$IYEAR <- brfss$IYEAR - 2020

summary(brfss$IYEAR)
write.csv(brfss, "final_data.csv", row.names = FALSE)
```

### Sampling
```{r}
set.seed(123)
sampled_data_2013_2022 <- brfss %>%
  filter(IYEAR >= -7 & IYEAR <= 2) %>%
  group_by(IYEAR) %>%
  sample_n(size = 1200)
sampled_data_2023 <- brfss %>%
  filter(IYEAR == 3) %>%
  sample_n(size = 100)

sampled_data <- bind_rows(sampled_data_2013_2022, sampled_data_2023)
```

### Checking Residual Distribution
```{r}
independent_variables <- colnames(sampled_data)[1:13]
for (variable in independent_variables) {
  model <- lm(paste('MENTHLTH', '~', variable), data = sampled_data)
  plot(model, which = 1, main = '')
  mtext(paste(variable),
        side = 3,
        line = 1,
        adj = 0.2)
}
```

### Correlation Table
```{r}
corr_matrix <- cor(sampled_data[1:13])
corrplot(
  corr_matrix,
  method = "color",
  addCoef.col = "black",
  tl.col = "black",
  type = "upper"
)
```

### Split Train & Test Set
```{r}
data_index <- sample(1:nrow(sampled_data), size = 0.7 * nrow(sampled_data))
train_set <- sampled_data[data_index,]
test_set <- sampled_data[-data_index,]
x <- as.matrix(train_set[, 1:13])
y <- as.matrix(train_set[, 14])
x_test <- as.data.frame(test_set[, 1:13])
y_test <- as.data.frame(test_set[, 14])
```

### Function for Prediction Performance
```{r}
predict_performance <- function(predictions, true_values) {
  # Check MAE, MSE, R-squared:
  mse <- mean((true_values - predictions) ^ 2)
  mae <- mean(abs(true_values - predictions))
  r_squared <- 1 - sum((true_values - predictions) ^ 2) / sum((true_values - mean(true_values)) ^ 2)
  mbd <- mean((predictions - true_values) / true_values) * 100
  result <- data.frame(Test = c("MSE", "MAE", "MBD", "R-Squared"),
                       Output = round(c(mse, mae, mbd, r_squared), 4))
  return(result)
}
```

### Linear Regression function in Rcpp
```{Rcpp}
// [[Rcpp::depends(Rcpp)]]
#include <Rcpp.h>
#include <vector>
#include <iostream>
//#include "fit_mlr.h"

std::vector<std::vector<double>> inverseMatrix(const std::vector<std::vector<double>>& M) {
  int n = M.size();
  std::vector<std::vector<double>> matrix(n, std::vector<double>(2*n, 0.0));
  for (int i=0; i < n; ++i) {
    matrix[i][i+n] = 1;
    for (int j = 0; j < n; ++j) {
      matrix[i][j] = M[i][j];
    }
  }
  for (int i=0; i < n; ++i) {
    double pivot = matrix[i][i];
    for (int j=0; j < 2*n; ++j) {
      matrix[i][j] /= pivot;
    }

    for (int k=0; k < n; ++k) {
      if (k!=i) {
        double factor = matrix[k][i];
        for (int j=0; j < 2*n; ++j) {
          matrix[k][j] -= factor*matrix[i][j];
        }
      }
    }
  }
  std::vector<std::vector<double>> inverse(n, std::vector<double>(n, 0.0));
  for (int i = 0; i < n; ++i) {
    for (int j = 0; j < n; ++j) {
      inverse[i][j] = matrix[i][j + n];
    }
  }
  return inverse;
}
// [[Rcpp::export]]
Rcpp::NumericVector fit_mlr(Rcpp::NumericVector y, Rcpp::NumericMatrix x) {
  if (x.nrow() == y.size()) {
    size_t var = x.ncol() + 1;
    Rcpp::NumericMatrix X(x.nrow(), var);
    for (size_t i=0; i<x.nrow(); ++i) {
      X(i, 0) = 1;
      for (int j=1; j<var; ++j) {
        X(i, j) = x(i, j-1);
      }
    }
    std::vector<std::vector<double>> XTX(var, std::vector<double>(var, 0.0));
    for (int i = 0; i < var; ++i) {
      for (int j = 0; j < var; ++j) {
        for (int k = 0; k < x.nrow(); ++k) {
          XTX[i][j] += X(k, i) * X(k, j);
        }
      }
    }
    std::vector<double> XTY(var, 0.0);
    for (int i = 0; i < var; ++i) {
      for (int j = 0; j < x.nrow(); ++j) {
        XTY[i] += X(j, i) * y(j);
      }
    }
    std::vector<std::vector<double>> inverse_XTX = inverseMatrix(XTX);
    std::vector<double> estimators(var, 0.0);
    for (int i=0; i<var; ++i) {
      for (int j=0; j<var; ++j) {
        estimators[i] += inverse_XTX[i][j]*XTY[j];
      }
    }
    Rcpp::NumericVector estimators_list(estimators.begin(), estimators.end());
    return estimators_list;
  }
  else {
    std::cout<<"length are not the same"<<std::endl;
    return NULL;
  }
}
```

### Multiple Linear Regression Model
```{r}
# Original method: with log(SMOKE100), log(EXERANY2), log(MARITAL), log(NUMADULT)
mlr_mod_based <- lm(MENTHLTH ~ IYEAR + EDUCA + log(SMOKE100) + log(EXERANY2) + log(MARITAL) + log(NUMADULT) + RENTHOM1 + HLTHPLN1 + EMPLOY1 + PVTRESD1 + MEDCOST + CHILDREN + INCOME2, data = train_set
)
summary(mlr_mod)
vif(mlr_mod)

# Improved method:
coefficients <- fit_mlr(y, x)
names(coefficients) <- c("(Intercept)", colnames(x))
print(coefficients)
mlr_predictions2 <- cbind(1, as.matrix(x_test)) %*% coefficients

system.time({
  predictions_basic <- predict(mlr_mod, newdata = test_set)
  summary(mlr_mod)
}) -> time_mlr

system.time({
  predictions_improved <- cbind(1, as.matrix(x_test)) %*% coefficients
}) -> time_rcpp_mlr

mlr_predictions <- predict(mlr_mod, newdata = test_set)
print(predict_performance(mlr_predictions, test_set$MENTHLTH))
```

### Ridge Regression Model
```{r}
r1 <- glmnet(x = x, y = y, family = "gaussian", alpha = 0)
plot(r1, xvar = "lambda")
r1cv <- cv.glmnet(x = x, y = y, family = "gaussian", alpha = 0, nfolds = 10)
plot(r1cv)
rmin <- glmnet(x = x, y = y, family = "gaussian", alpha = 0,lambda = r1cv$lambda.min)
coef(rmin)
r1se <- glmnet(x = x, y = y, family = "gaussian", alpha = 0,lambda = r1cv$lambda.1se)
coef(r1se)
ridge_mod <- linearRidge(MENTHLTH ~ ., data = train_set, lambda = r1cv$lambda.min)

rd_predictions <- predict(ridge_mod, newdata = test_set)
print(predict_performance(rd_predictions, test_set$MENTHLTH))
```

### Random Forest Model
```{r}
#Parallel
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
system.time({
    rf_model_parallel <- randomForest(MENTHLTH ~ ., data = train_set, ntree=800, importance = TRUE, proximity = TRUE)}) -> rf_parallel_time
stopCluster(cl)

brfss_rf=randomForest(MENTHLTH~., data = train_set, ntree = 800, importance = TRUE, proximity = TRUE)
plot(brfss_rf)

#Original
system.time({
  model_base <- randomForest(MENTHLTH ~ ., data = train_set, ntree=800, importance = TRUE, proximity = TRUE)
}) -> time_base_rf

#Spark
#spark 46.35
spark_available_versions()
spark_install(version = 3.3)
sc <- spark_connect(master = "local")
options(rstudio.connectionObserver.errorsSuppressed = TRUE)
options("sparklyr.simple.errors" = TRUE)
sc <- spark_connect(master = "local", config = list("spark.executor.memory" = "4g"))
sdf <- copy_to(sc, data, "data", overwrite = TRUE)
train_sdf <- copy_to(sc, train_set, "train_set", overwrite = TRUE)
test_sdf <- copy_to(sc, test_set, "test_set", overwrite = TRUE)

system.time({
  rf_model_spark <- train_sdf %>%
    ml_random_forest(MENTHLTH ~ .)
}) -> time_spark_rf

spark_disconnect(sc)

# Check the importance
importance(brfss_rf)

rf_predictions <- predict(brfss_rf, newdata = test_set)
print(predict_performance(rf_predictions, test_set$MENTHLTH))
```

### efficiency_comparison
```{r}
# For MLR
efficiency_comparison_mlr <- data.frame(
  Linear_Regression <- c("Original", "Rcpp"),
  Time <- c(time_mlr[3], time_rcpp_mlr[3])
)
print(efficiency_comparison_mlr)

# For Random Forest
efficiency_comparison_rf <- data.frame(
  Random_Forest <- c("Original", "Parallel", "Spark"),
  Time <- c(time_base_rf[3], rf_parallel_time[3], time_spark_rf[3])
)
print(efficiency_comparison_rf)
```
